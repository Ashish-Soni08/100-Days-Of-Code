{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashish-Soni08/100-Days-Of-Code/blob/main/course/week2/DCDL_Week_2_Tensor_Playground_Ashish_Soni.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Playground\n",
        "\n",
        "A quick bag of tips for working with Tensors. "
      ],
      "metadata": {
        "id": "gNVkdkZsK5Ch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cubPq0lyKmiq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the best practices for DL programming is to annotate expected shape as you go. Here's an example:"
      ],
      "metadata": {
        "id": "Cd_L4XuqUm_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 3)  # shape: 2 x 3\n",
        "x = x[0] * x[1] # shape: 3\n",
        "x = x.unsqueeze(1) # shape: 3 x 1"
      ],
      "metadata": {
        "id": "8JHEr8uDK3fp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This makes it a lot easier for the reader to keep track of what is going on."
      ],
      "metadata": {
        "id": "IbBCEfHeVMum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why is shape even important?"
      ],
      "metadata": {
        "id": "VerEjvXtVQhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One big reason is that we often want to do vector and matrix multiplications in deep learning. If the shapes aren't exactly matching, then unexpected things might happen. Here are some examples:"
      ],
      "metadata": {
        "id": "eIsHWyepVQjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We expect a shape of (2, 1)\n",
        "A = torch.randn(2, 3)  # shape: 2 x 3\n",
        "B = torch.randn(3, 1)  # shape: 3 x 1\n",
        "C = A @ B              # shape: 2 x 1\n",
        "print(C.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fAIgeFtXTcT",
        "outputId": "433a75ed-fbeb-4ec7-d455-1104c54a88ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we pass a vector as the second argument, it will not error out. \n",
        "# Rather it performs a matrix vector multiplication, returning a vector\n",
        "A = torch.randn(2, 3)  # shape: 2 x 3\n",
        "B = torch.randn(3)     # shape: 3\n",
        "C = A @ B              # shape: 2\n",
        "print(C.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2lniL1QXTVx",
        "outputId": "fbbe9fe4-cd6c-454c-c8cc-25e1e20442a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we pass a vector as the first argument, it also will not error out. \n",
        "# It will again do vector-matrix multiplication, returning a vector\n",
        "A = torch.randn(3)     # shape: 3\n",
        "B = torch.randn(3, 2)  # shape: 3 x 2\n",
        "C = A @ B              # shape: 2\n",
        "print(C.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqmkchdBUvi1",
        "outputId": "962d9f26-a7b1-4cad-dacb-6c07c2937c05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These ideas translate to larger vectors of course\n",
        "# Below, we basically \"ignore\" the first dimension of A and \n",
        "# perform a matrix-matrix multiplication (2,3) times (3,5) a total\n",
        "# of 16 times, resulting in sixteen (2,5) matrices\n",
        "A = torch.randn(16, 2, 3)   # shape: 16 x 2 x 3\n",
        "B = torch.randn(3, 5)       # shape: 3 x 5\n",
        "C = A @ B                   # shape: 16 x 2 x 5\n",
        "C.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzupX_FCYdIa",
        "outputId": "cc554875-5939-4d38-8275-cfba1ca4f5ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 2, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The first dimension often has the meaning of being batch size. \n",
        "# In these cases, we might want to do \"batch matrix multiplications\"\n",
        "mb = 16\n",
        "A = torch.randn(mb, 2, 3)  # shape: mb x 2 x 3\n",
        "B = torch.randn(mb, 3, 5)  # shape: mb x 3 x 5\n",
        "# `bmm` stands for batch matrix multiplication\n",
        "# You can also use `torch.matmul` or `torch.einsum`.\n",
        "C = torch.bmm(A, B)        # shape: mb x 2 x 5\n",
        "C.size()  # this is different than above!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_DVkvMyY52f",
        "outputId": "701e786d-1ad0-4745-a1c1-6770aa348a1c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 2, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other reason is that library functions like objective functions expect very specific shapes. We can see documentation of this online but it is important to make sure shapes match. Otherwise, again unexpected things happen."
      ],
      "metadata": {
        "id": "ouyuXTsHVa_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here is a tricky one\n",
        "x = torch.randn(3)\n",
        "total = torch.sum(x)  # what shape is total?"
      ],
      "metadata": {
        "id": "T0Wg2HVcVxr2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total.size()  # blank!\n",
        "print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc0WycVwaGqv",
        "outputId": "a9fb3c0b-45d3-4fa9-953c-02ca07c2636a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-1.2011)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a special torch Tensor. You might expect summing a vector to return a float, but this is not a Python float. It is a \"0-dim\" torch Tensor containing only one number. "
      ],
      "metadata": {
        "id": "AmgzXvXhaKLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = total.unsqueeze(0)  # for example we can add a dimension with unsqueeze\n",
        "total.size()                # shape: 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKT8hdvLaJeR",
        "outputId": "71c96199-46a1-44a5-96ea-1e38500411a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common case when we need to care about shape is for loss functions. For example, let's look at binary cross entropy and cross entropy. "
      ],
      "metadata": {
        "id": "DHfXnBHTairi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy on a 3-way classification problem\n",
        "\n",
        "num_class = 3\n",
        "num_examples = 64\n",
        "\n",
        "logits = torch.randn(num_examples, num_class)  # shape: 64 x 3\n",
        "labels = torch.randint(0, 3, (num_examples,))  # shape: 64\n",
        "\n",
        "loss = F.cross_entropy(logits, labels)         # shape: 0\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33p4bIyrao_o",
        "outputId": "194cd451-2c79-45ba-87d5-375915775cb9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.3246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary cross-entropy on a 2-way classification problem\n",
        "\n",
        "num_examples = 64\n",
        "\n",
        "logits = torch.randn(num_examples, 1)                      # shape: 64 x 1\n",
        "labels = torch.randint(0, 1, (num_examples,))              # shape: 64\n",
        "# read the documentation! This loss function expects labels to be (N, 1) shape\n",
        "# and expects it to be a FloatTensor, not a LongTensor. \n",
        "labels = labels.unsqueeze(1).float()                       # shape: 64 x 1\n",
        "loss = F.binary_cross_entropy_with_logits(logits, labels)  # shape: 0\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_qBkkAIbHIn",
        "outputId": "6d232523-eca2-4edb-c4c6-1fa5615ab4b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7597)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Operations\n",
        "\n",
        "Four functions you will use a lot in PyTorch are `squeeze`, `unsqueeze`, `repeat`, and `view`. These are quite universal functions. In other languages like Tensorflow or Matlab, these functions may not be called the same thing but the same functions exist. "
      ],
      "metadata": {
        "id": "gxrSy7rpVyev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `squeeze(dim)` removes a dimension at index `dim`. \n",
        "# But it can only do so if the dimension at `dim` is size 1. \n",
        "x = torch.randn(16, 5, 2, 3)  # shape: 16 x 5 x 2 x 3\n",
        "print(x.squeeze(0).size())    # does nothing!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFg-gLBZV2kP",
        "outputId": "e4441434-f442-48ee-f037-eb651e0f615e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 5, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(16, 1, 2, 3)  # shape: 16 x 1 x 2 x 3\n",
        "print(x.squeeze(1).size())    # shape: 16 x 2 x 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E9JaGrCb7UT",
        "outputId": "c856fd1d-61d6-4ac7-df84-f42b93020528"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you don't pass a `dim`, squeeze will get rid of all \"useless\" dimensions\n",
        "x = torch.randn(16, 1, 1, 1)  # shape: 16 x 1 x 1 x 1\n",
        "print(x.squeeze().size())     # shape: 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZzQwow1cIc8",
        "outputId": "b4912b69-5b40-465c-f862-b0a32742af08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# `unsqueeze(dim)` does the opposite. It adds a dimension of 1 at a position `dim`\n",
        "x = torch.randn(16, 5, 2, 3)  # shape: 16 x 5 x 2 x 3\n",
        "print(x.unsqueeze(0).size())  # shape: 1 x 16 x 5 x 2 x 3\n",
        "print(x.unsqueeze(1).size())  # shape: 16 x 1 x 5 x 2 x 3\n",
        "print(x.unsqueeze(2).size())  # shape: 16 x 5 x 1 x 2 x 3\n",
        "print(x.unsqueeze(3).size())  # shape: 16 x 5 x 2 x 1 x 3\n",
        "print(x.unsqueeze(4).size())  # shape: 16 x 5 x 2 x 3 x 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQYFz84qcUSM",
        "outputId": "80312e5d-d963-4c0f-87f6-fb9a32201ee1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 5, 2, 3])\n",
            "torch.Size([16, 1, 5, 2, 3])\n",
            "torch.Size([16, 5, 1, 2, 3])\n",
            "torch.Size([16, 5, 2, 1, 3])\n",
            "torch.Size([16, 5, 2, 3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A common operation you might see is to unsqueeze and then repeat\n",
        "x = torch.randn(16)                        # shape: 16\n",
        "print(x.unsqueeze(1).repeat(1, 5).size())  # shape: 16 x 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy4Q7oWQck8b",
        "outputId": "63f3b3d7-daaf-49ae-ff43-9e25f7b0e9d1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, a popular thing is to reshape tensor sizes. \n",
        "x = torch.randn(120)             # shape: 120\n",
        "print(x.view(12, 2, 5).size())   # shape: 12 x 2 x 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2OQ5SPKcwE0",
        "outputId": "00033774-50a7-4a2a-d8cb-f634cc22187a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 2, 5])\n"
          ]
        }
      ]
    }
  ]
}